{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surendra123m/FMML_2023_PROGRAM_SOLVING/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92bf1e01-f120-4df1-833d-78e9a53f5277"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8407582a-1180-4a46-e57c-08db413deece"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55639869-e895-43aa-8d2f-5c954aeaa383"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f816e0-b624-4283-b872-ae95e3ad7f0b"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2632e77c-5264-488f-df8c-f56e81c9c407"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9dfffe6-6010-4362-a5ec-fede09513090"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "\n",
        "The percentage of the validation set in a machine learning or deep learning experiment can significantly impact the performance evaluation of your model. Here's how it's affected when you increase or decrease the percentage of the validation set:\n",
        "\n",
        "Increase the Percentage of Validation Set:\n",
        "\n",
        "Pros:\n",
        "More Reliable Evaluation: With a larger validation set, you get a more reliable estimate of your model's performance. This is particularly useful when your dataset is large, and you can afford to allocate more data to validation.\n",
        "Reduced Overfitting Risk: A larger validation set can help you detect overfitting more effectively. Overfitting occurs when your model learns to perform well on the training data but doesn't generalize to new, unseen data. A larger validation set makes it harder for a model to overfit since it needs to perform well on a larger variety of examples.\n",
        "Cons:\n",
        "Reduced Training Data: By allocating more data to the validation set, you have less data available for training your model. This can be a problem when you have a limited dataset, as it might lead to underfitting.\n",
        "Decrease the Percentage of Validation Set:\n",
        "\n",
        "Pros:\n",
        "More Data for Training: Allocating a smaller percentage to the validation set means you have more data available for training your model. This can be beneficial when you have a limited dataset.\n",
        "Faster Training: With less data in the validation set, your model's training process may be faster, allowing you to experiment with different hyperparameters or model architectures more quickly.\n",
        "Cons:\n",
        "Less Reliable Evaluation: A smaller validation set may lead to a less reliable estimate of your model's performance. The evaluation may be more sensitive to variations in the validation data, and the reported performance metrics may not be representative of how well your model generalizes to new, unseen data.\n",
        "Higher Risk of Overfitting: With a smaller validation set, it's easier for a model to overfit since it has less data to generalize from. This can result in a model that performs well on the validation set but poorly on new data."
      ],
      "metadata": {
        "id": "yIX853_-Rt4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3anu5hnGSYNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "\n"
      ],
      "metadata": {
        "id": "8TC5wzkcSdEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the training and validation sets can significantly affect how well you can predict the accuracy on the test set using the validation set. This relationship is influenced by several factors:\n",
        "\n",
        "Representativeness of the Validation Set:\n",
        "\n",
        "If the validation set is not representative of the test set in terms of data distribution, size, or difficulty, then the accuracy on the validation set may not be a good indicator of performance on the test set. For instance, if the validation set is very small or lacks diversity, it might not capture the full range of scenarios that the test set contains.\n",
        "Sample Variability:\n",
        "\n",
        "A larger validation set is generally better at capturing the variability present in the dataset. It provides a more stable estimate of your model's performance because it's less sensitive to random fluctuations in the data. This can lead to more accurate predictions of how well your model will perform on unseen data.\n",
        "Overfitting and Underfitting:\n",
        "\n",
        "The size of the training set can influence the risk of overfitting or underfitting. If you have a very small training set, your model might overfit, which means it memorizes the training data but doesn't generalize well. In this case, the validation accuracy may be deceptively high. Conversely, if the training set is too large, it can help the model generalize better, but it might also make it harder to detect overfitting.\n",
        "Statistical Confidence:\n",
        "\n",
        "A larger validation set allows for more statistically confident performance estimation. With a small validation set, you may have a higher chance of getting misleading results due to random variation. A larger validation set reduces this risk and provides a more reliable estimate of model performance.\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "When tuning hyperparameters (e.g., learning rate, regularization strength), a larger validation set can be more informative. It allows for a better assessment of how different hyperparameters impact model performance. In contrast, a small validation set may not provide enough data points for robust hyperparameter tuning.\n",
        "Computational Resources:\n",
        "\n",
        "The size of the validation set can also affect the speed of experimentation. A smaller validation set can lead to faster model training and evaluation, which can be advantageous when you have limited computational resources or time constraints."
      ],
      "metadata": {
        "id": "_4-nLGRpS0Ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "The percentage of data to reserve for the validation set should be chosen carefully to strike a balance between having a representative sample for model evaluation and having enough data for model training. While there is no one-size-fits-all answer, a common practice is to allocate around 10-20% of your total dataset for validation when you have a reasonably sized dataset.\n",
        "\n",
        "Here are some considerations for choosing the percentage of data for the validation set:\n",
        "\n",
        "Dataset Size: The total size of your dataset is a crucial factor. If you have a large dataset (tens of thousands of samples or more), you can afford to allocate a smaller percentage to the validation set (e.g., 10%). In contrast, with a smaller dataset, you may want to allocate a larger percentage (e.g., 20%) to ensure you have enough data for reliable validation.\n",
        "\n",
        "Complexity of the Problem: More complex problems might require larger validation sets. If your problem involves a high degree of variability or if it's challenging for your model to generalize, a larger validation set can help capture this complexity.\n",
        "\n",
        "Resource Constraints: Consider your available computational resources. A smaller validation set can lead to faster model training and experimentation, which can be advantageous when you have limited resources or need to iterate quickly.\n",
        "\n",
        "Cross-Validation: If you are concerned about the balance between validation and training data, you can employ cross-validation techniques. For example, k-fold cross-validation divides your dataset into k subsets, using each as a validation set while training on the remaining data. This helps you use your data more efficiently and provides multiple estimates of model performance.\n",
        "\n",
        "Domain Knowledge: Your domain expertise can guide your decision. Some domains may require specific validation set sizes based on known characteristics of the data or problem."
      ],
      "metadata": {
        "id": "tP9qLOesTC7w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3bd116-38d8-4337-b1a3-3b89c0d2033b"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Does averaging the validation accuracy across multiple splits give more consistent results?"
      ],
      "metadata": {
        "id": "BIvnKgDJTUUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, averaging the validation accuracy across multiple splits, such as in k-fold cross-validation, generally provides more consistent and robust results compared to a single validation split. Here's why averaging validation accuracy across multiple splits is beneficial:\n",
        "\n",
        "Reduced Variability: Averaging over multiple splits reduces the impact of random variations in the data that can affect the performance estimate. In a single validation split, the choice of which data points end up in the validation set can influence the result significantly. With multiple splits, these variations tend to balance out, resulting in a more stable and reliable estimate of model performance.\n",
        "\n",
        "Better Generalization Assessment: By repeatedly splitting the data into different subsets, cross-validation provides a more comprehensive assessment of how well your model generalizes to various parts of the dataset. This helps ensure that your performance estimate is representative of the model's ability to perform well on unseen data.\n",
        "\n",
        "Mitigating Data Imbalance: In cases where your dataset is imbalanced (i.e., one class is much more prevalent than others), cross-validation ensures that each class has a chance to appear in both training and validation sets across multiple folds. This prevents the model from learning biases associated with imbalanced datasets.\n",
        "\n",
        "More Efficient Data Utilization: Cross-validation makes more efficient use of your dataset by repeatedly using each data point for both training and validation, reducing the risk of wasting valuable data.\n",
        "\n",
        "Hyperparameter Tuning: When tuning hyperparameters or selecting the best model among several candidates, cross-validation provides a more robust basis for comparison. It allows you to assess how well each model configuration generalizes across multiple data subsets.\n",
        "\n",
        "Statistical Significance: Averaging results over multiple folds also allows you to assess statistical significance. You can compute confidence intervals or perform hypothesis tests to determine if differences in performance between models or configurations are statistically significant."
      ],
      "metadata": {
        "id": "2VW-b_xnTeb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 Does it give more accurate estimate of test accuracy?\n",
        "\n",
        "Averaging the validation accuracy across multiple splits in techniques like k-fold cross-validation provides a more accurate estimate of the model's generalization performance compared to a single validation split. However, it's important to note that the test accuracy and the cross-validation accuracy are not the same thing.\n",
        "\n",
        "Here's a breakdown of how these concepts relate:\n",
        "\n",
        "Validation Accuracy (Cross-Validation Accuracy): This is an estimate of how well your model is expected to perform on new, unseen data based on the validation splits within your training data. When you perform k-fold cross-validation, you compute multiple validation accuracies, one for each fold. Averaging these validation accuracies provides a more robust estimate of your model's performance on unseen data compared to a single validation split.\n",
        "\n",
        "Test Accuracy: This is the performance metric you obtain when you evaluate your trained model on a completely independent and previously unseen dataset, often referred to as the \"test set.\" The test set is distinct from your training and validation data and serves as a final and realistic assessment of how well your model generalizes to new, real-world examples.\n",
        "\n",
        "While cross-validation helps you estimate how well your model might perform on new data based on your training data, the test accuracy provides a more accurate and final evaluation because it evaluates the model on data it has never seen during training or validation. The test accuracy is often considered the gold standard for assessing model performance because it simulates how the model will perform in real-world scenarios.\n",
        "\n",
        "In summary, cross-validation, by averaging validation accuracies across multiple splits, provides a more reliable estimate of model generalization compared to a single validation split. However, the true and final measure of a model's performance on new, unseen data is the test accuracy, which should be evaluated on an independent test set that the model has not been exposed to during training or validation."
      ],
      "metadata": {
        "id": "a6lID4jCTrBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "\n",
        "The number of iterations or folds in techniques like k-fold cross-validation can have an impact on the accuracy of your performance estimate. Here's how it affects the estimate, and whether a higher number of iterations results in a better estimate:\n",
        "\n",
        "Effect of Number of Iterations (Folds):\n",
        "\n",
        "More Iterations (Folds): Increasing the number of iterations (i.e., using more folds) in cross-validation generally leads to a more accurate and robust estimate of model performance. This is because more iterations provide more opportunities for the model to be trained and evaluated on different subsets of the data.\n",
        "Fewer Iterations (Folds): Using fewer iterations can still provide an estimate of performance but may be more sensitive to the specific random splits of the data. It may result in a less stable estimate, especially when the dataset is relatively small.\n",
        "Bias-Variance Trade-off:\n",
        "\n",
        "A larger number of iterations in cross-validation reduces the bias in the performance estimate. Bias refers to systematic errors in the estimate, and with more iterations, the estimate tends to be closer to the true performance of the model.\n",
        "However, there is a trade-off with variance. More iterations can increase the variance in the estimate, as each fold provides a different estimate, and averaging more estimates can result in a wider range of values. This is generally not a problem as long as you have a sufficiently large dataset.\n",
        "Computational Cost:\n",
        "\n",
        "Increasing the number of iterations also increases the computational cost of cross-validation, as the model needs to be trained and evaluated multiple times. You should consider your available computational resources when choosing the number of iterations.\n",
        "Optimal Number of Iterations:\n",
        "\n",
        "There is no one-size-fits-all answer for the optimal number of iterations. The choice of the number of iterations depends on factors such as the size of your dataset, the computational resources available, and the stability of the performance estimate you require.\n",
        "Common choices for the number of iterations in k-fold cross-validation are 5-fold and 10-fold, but you can experiment with different values to see how they affect your results. In some cases, leave-one-out cross-validation (where each sample is its own fold) is used for small datasets."
      ],
      "metadata": {
        "id": "HlKy3wtCUCcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n",
        "\n",
        "Increasing the number of iterations (folds) in cross-validation can help mitigate the impact of having a very small train or validation dataset to some extent, but it doesn't completely solve the underlying issues associated with limited data. Here's how increasing the iterations can help and the limitations:\n",
        "\n",
        "Advantages of Increasing Iterations:\n",
        "\n",
        "Reduced Variability: When you have a very small training or validation dataset, there's a higher risk of getting unrepresentative splits or experiencing greater variability in performance estimates. Increasing the number of iterations can help reduce this variability by considering multiple different data partitions.\n",
        "\n",
        "More Stable Estimates: With more iterations, you get a more stable and reliable estimate of model performance. This can be especially useful when dealing with limited data because it allows you to assess how well your model generalizes across different subsets of the small dataset.\n",
        "\n",
        "Limitations and Considerations:\n",
        "\n",
        "Limited Data: Increasing the number of iterations does not magically create more data. It only provides more ways to sample and assess the same limited data. If your training dataset is very small, your model's ability to generalize effectively might still be compromised.\n",
        "\n",
        "Computation Cost: While increasing iterations helps with performance estimation, it also increases computational cost. If you have extremely limited computational resources, this approach may not be feasible.\n",
        "\n",
        "Risk of Overfitting: When dealing with a very small dataset, there's an increased risk of overfitting. Cross-validation can help in estimating performance, but it may not fully address the underlying issue of model generalization if the dataset size is severely limited. You may need to consider strategies like simplifying the model or using regularization techniques.\n",
        "\n",
        "Data Imbalance: If the small dataset is imbalanced (i.e., some classes have very few examples), increasing iterations can help ensure that each class appears in both training and validation sets more frequently, reducing the risk of biased performance estimates.\n",
        "\n",
        "In situations where you have an extremely small dataset, it's essential to be cautious and consider other techniques beyond just increasing iterations. You might explore data augmentation (creating synthetic data), transfer learning (leveraging pre-trained models), or using simpler model architectures to combat overfitting. Additionally, collecting more data, if possible, remains one of the most effective ways to address the challenges associated with very small datasets.\n"
      ],
      "metadata": {
        "id": "0XBGLcYSUZwL"
      }
    }
  ]
}